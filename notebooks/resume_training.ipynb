{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caab5a80",
   "metadata": {},
   "source": [
    "# Symbol Detection - Resume Training on Google Colab\n",
    "\n",
    "Clean notebook optimized for **resuming training** from existing checkpoints. Designed to avoid the dependency issues of the original notebook.\n",
    "\n",
    "## ‚ö° Quick Start:\n",
    "1. Run cells 1-3 (setup & install)\n",
    "2. **‚ö†Ô∏è Restart Runtime** (Runtime ‚Üí Restart session) \n",
    "3. After restart: Run cells 1-2 again, **skip cell 3**, continue from cell 4\n",
    "4. Configure resume settings in cell 7\n",
    "5. Run training (cell 8)\n",
    "\n",
    "**Note**: The runtime restart after installation is REQUIRED to fix numpy/torchvision compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc68e9c",
   "metadata": {},
   "source": [
    "## 1. Environment Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540a5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENVIRONMENT DETECTION\n",
      "======================================================================\n",
      "Running on: Google Colab\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 79.3 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Running on: {'Google Colab' if IN_COLAB else 'Local Machine'}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"GPU: Not available (will use CPU - training will be slow!)\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0bc8715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úì Google Drive mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive if on Colab\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"‚úì Google Drive mounted at /content/drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not mount Google Drive: {e}\")\n",
    "        print(\"Proceeding without Drive - checkpoints will save locally\")\n",
    "else:\n",
    "    print(\"Running locally - not attempting Drive mount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63880ae9",
   "metadata": {},
   "source": [
    "## 2. Clone/Setup Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd20788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists, pulling latest changes...\n",
      "‚úì Repository path: /content/symbol-detection\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "if IN_COLAB:\n",
    "    repo_path = '/content/symbol-detection'\n",
    "    if not os.path.exists(repo_path):\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/BhanukaDev/symbol-detection.git', repo_path], check=True)\n",
    "    else:\n",
    "        print(\"Repository already exists, pulling latest changes...\")\n",
    "        os.chdir(repo_path)\n",
    "        subprocess.run(['git', 'pull'], check=True)\n",
    "else:\n",
    "    # Local development - repository should already be present\n",
    "    repo_path = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "    \n",
    "print(f\"‚úì Repository path: {repo_path}\")\n",
    "os.chdir(repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e58a3",
   "metadata": {},
   "source": [
    "## 3. Dependency Installation with Version Lock\n",
    "\n",
    "**This cell fixes the numpy/PyTorch compatibility issue that broke the previous notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a75fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies for Google Colab...\n",
      "======================================================================\n",
      "Step 1: Installing numpy==1.26.4 (locked version)...\n",
      "\n",
      "Step 2: Installing PyTorch ecosystem...\n",
      "\n",
      "Step 3: Installing local packages...\n",
      "\n",
      "======================================================================\n",
      "‚úì Dependencies installed successfully\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: You MUST restart the runtime now!\n",
      "Go to: Runtime ‚Üí Restart session\n",
      "\n",
      "This is CRITICAL - torchvision needs to reload with the correct numpy version.\n",
      "After restart, skip this cell and run from the next cell.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Installing dependencies for Google Colab...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Install numpy first with exact version\n",
    "    print(\"Step 1: Installing numpy==1.26.4 (locked version)...\")\n",
    "    os.system(\"pip install --no-cache-dir --force-reinstall numpy==1.26.4\")\n",
    "    \n",
    "    # Step 2: Install PyTorch and dependencies\n",
    "    print(\"\\nStep 2: Installing PyTorch ecosystem...\")\n",
    "    os.system(\"pip install --no-cache-dir torch torchvision torchmetrics pycocotools timm\")\n",
    "    \n",
    "    # Step 3: Install local packages\n",
    "    print(\"\\nStep 3: Installing local packages...\")\n",
    "    os.chdir(f'{repo_path}/python')\n",
    "    os.system(\"pip install --no-cache-dir -e ./floor-grid --no-deps\")\n",
    "    os.system(\"pip install --no-cache-dir -e ./effects --no-deps\")\n",
    "    os.system(\"pip install --no-cache-dir -e . --no-deps\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì Dependencies installed successfully\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n‚ö†Ô∏è  IMPORTANT: You MUST restart the runtime now!\")\n",
    "    print(\"Go to: Runtime ‚Üí Restart session\")\n",
    "    print(\"\\nThis is CRITICAL - torchvision needs to reload with the correct numpy version.\")\n",
    "    print(\"After restart, skip this cell and run from the next cell.\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"Running locally - skipping Colab-specific installation\")\n",
    "    print(\"Make sure you have dependencies installed: pip install -e ./python[dev]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7869e",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è STOP HERE - Restart Runtime Required!\n",
    "\n",
    "After running the cell above, you **MUST** restart the runtime:\n",
    "1. Go to: **Runtime ‚Üí Restart session**\n",
    "2. After restart, **skip cells 2-3** and continue from cell 4 (Package Verification) below\n",
    "\n",
    "This restart is necessary for torchvision to reload with the correct numpy version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf416f",
   "metadata": {},
   "source": [
    "## 4. Package Import Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8d9f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PACKAGE VERIFICATION\n",
      "======================================================================\n",
      "Numpy version: 1.26.4\n",
      "Verifying package imports...\n",
      "‚úì symbol_detection.training\n",
      "‚úì COCODetectionDataset (training.data)\n",
      "\n",
      "‚úì All required packages imported successfully\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PACKAGE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check numpy version first\n",
    "print(f\"Numpy version: {numpy.__version__}\")\n",
    "if not numpy.__version__.startswith(\"1.26\"):\n",
    "    print(\"‚ùå Wrong numpy version detected!\")\n",
    "    print(\"You MUST restart the runtime (Runtime ‚Üí Restart session) after installing dependencies.\")\n",
    "    print(\"Then skip the installation cell and run from here.\")\n",
    "    raise RuntimeError(f\"Numpy {numpy.__version__} detected, need 1.26.x. Restart runtime!\")\n",
    "\n",
    "# Ensure Python path includes the source directory\n",
    "python_path = f'{repo_path}/python/src'\n",
    "if python_path not in sys.path:\n",
    "    sys.path.insert(0, python_path)\n",
    "\n",
    "print(\"Verifying package imports...\")\n",
    "try:\n",
    "    from symbol_detection.training import Trainer, CIoULoss\n",
    "    from symbol_detection.training.data import COCODetectionDataset\n",
    "    print(\"‚úì symbol_detection.training\")\n",
    "    print(\"‚úì COCODetectionDataset (training.data)\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚úó Import error: {e}\")\n",
    "    print(\"\\nDid you restart the runtime after installing dependencies?\")\n",
    "    print(\"Go to: Runtime ‚Üí Restart session, then skip installation and re-run from here\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úì All required packages imported successfully\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c27c3",
   "metadata": {},
   "source": [
    "## 5. Dataset and Checkpoint Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd63df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Google Drive for storage\n",
      "\n",
      "Dataset directory: /content/drive/MyDrive/symbol-detection/dataset\n",
      "Checkpoints directory: /content/drive/MyDrive/symbol-detection/checkpoints\n",
      "\n",
      "‚úì Found 20 existing checkpoints:\n",
      "  - model_epoch_330.pth (315.0 MB)\n",
      "  - model_epoch_340.pth (315.0 MB)\n",
      "  - model_epoch_50.pth (315.0 MB)\n",
      "  - model_epoch_80.pth (315.0 MB)\n",
      "  - model_epoch_final.pth (315.0 MB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fast storage toggle: use /content local disk (much faster than Drive)\n",
    "USE_LOCAL_FAST = True if IN_COLAB else False\n",
    "\n",
    "DRIVE_MOUNTED = os.path.exists('/content/drive/MyDrive') if IN_COLAB else False\n",
    "\n",
    "if IN_COLAB and USE_LOCAL_FAST:\n",
    "    dataset_dir = Path('/content/symbol-detection/dataset')\n",
    "    checkpoints_dir = Path('/content/symbol-detection/checkpoints')\n",
    "    print(\"Using LOCAL /content storage (fast). Copy data/checkpoints here if needed.\")\n",
    "elif IN_COLAB and DRIVE_MOUNTED:\n",
    "    dataset_dir = Path('/content/drive/MyDrive/symbol-detection/dataset')\n",
    "    checkpoints_dir = Path('/content/drive/MyDrive/symbol-detection/checkpoints')\n",
    "    print(\"Using Google Drive storage (slower I/O)\")\n",
    "elif IN_COLAB:\n",
    "    dataset_dir = Path('/content/symbol-detection/dataset')\n",
    "    checkpoints_dir = Path('/content/symbol-detection/checkpoints')\n",
    "    print(\"Drive not mounted - using local /content storage\")\n",
    "else:\n",
    "    dataset_dir = Path(repo_path) / 'python' / 'dataset'\n",
    "    checkpoints_dir = Path(repo_path) / 'python' / 'checkpoints'\n",
    "    print(\"Using local storage\")\n",
    "\n",
    "# Create directories\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nDataset directory: {dataset_dir}\")\n",
    "print(f\"Checkpoints directory: {checkpoints_dir}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "checkpoints = list(checkpoints_dir.glob('*.pth'))\n",
    "if checkpoints:\n",
    "    print(f\"\\n‚úì Found {len(checkpoints)} existing checkpoints:\")\n",
    "    for ckpt in sorted(checkpoints)[-5:]:  # Show last 5\n",
    "        size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No checkpoints found in this location - copy them here if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9812a0b",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87a4dfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "num_epochs          : 50\n",
      "batch_size          : 8\n",
      "learning_rate       : 0.005\n",
      "num_classes         : 7\n",
      "use_ciou_loss       : True\n",
      "eval_every_n        : 10\n",
      "enable_ap_eval      : True\n",
      "\n",
      "Resume training: True\n",
      "Auto-detected: model_epoch_340.pth\n",
      "Extend to epoch: 400\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# CUDA memory optimization\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 50,          # Total epochs to train (can resume and extend)\n",
    "    'batch_size': 8,            # Reduced to prevent OOM errors\n",
    "    'learning_rate': 0.005,\n",
    "    'num_classes': 7,           # Electrical symbols\n",
    "    'use_ciou_loss': True,\n",
    "    'eval_every_n': 10,\n",
    "    'enable_ap_eval': True,\n",
    "}\n",
    "\n",
    "# Resume configuration (KEY PART)\n",
    "RESUME_TRAINING = True                              # Set to False to start from scratch\n",
    "RESUME_FROM_CHECKPOINT = None                       # Auto-detect latest or specify manually\n",
    "EXTEND_TRAINING_TO = 400                            # Extend training to this epoch number\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\nResume training: {RESUME_TRAINING}\")\n",
    "if RESUME_FROM_CHECKPOINT is None and RESUME_TRAINING:\n",
    "    # Auto-detect latest checkpoint\n",
    "    checkpoints = list(checkpoints_dir.glob('*.pth'))\n",
    "    if checkpoints:\n",
    "        RESUME_FROM_CHECKPOINT = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"Auto-detected: {RESUME_FROM_CHECKPOINT.name}\")\n",
    "    \n",
    "print(f\"Extend to epoch: {EXTEND_TRAINING_TO}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7045592",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer and Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf40dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n",
      "Using device: cuda\n",
      "‚úì Trainer initialized on device: cuda\n",
      "‚úì Model: ResNet50+FPN\n",
      "‚úì CIoU Loss: enabled\n",
      "\n",
      "Loading checkpoint: model_epoch_340.pth\n",
      "Loaded checkpoint: /content/drive/MyDrive/symbol-detection/checkpoints/model_epoch_340.pth\n",
      "‚úì Loaded epoch: 340\n",
      "‚úì Restored optimizer state\n",
      "‚úì Restored training history\n",
      "\n",
      "Extending training from 50 to 400 epochs\n"
     ]
    }
   ],
   "source": [
    "from symbol_detection.training import Trainer\n",
    "\n",
    "# Initialize trainer (with fresh model)\n",
    "print(\"Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    dataset_dir=str(dataset_dir),\n",
    "    output_dir=str(checkpoints_dir),\n",
    "    num_classes=TRAINING_CONFIG['num_classes'],\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    num_epochs=TRAINING_CONFIG['num_epochs'],\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    use_ciou_loss=TRAINING_CONFIG['use_ciou_loss'],\n",
    "    eval_every_n=TRAINING_CONFIG['eval_every_n'],\n",
    "    enable_ap_eval=TRAINING_CONFIG['enable_ap_eval'],\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer initialized on device: {trainer.device}\")\n",
    "print(f\"‚úì Model: ResNet50+FPN\")\n",
    "print(f\"‚úì CIoU Loss: enabled\")\n",
    "\n",
    "# Load checkpoint if resuming\n",
    "if RESUME_TRAINING and RESUME_FROM_CHECKPOINT:\n",
    "    print(f\"\\nLoading checkpoint: {RESUME_FROM_CHECKPOINT.name}\")\n",
    "    try:\n",
    "        trainer.load_checkpoint(str(RESUME_FROM_CHECKPOINT), resume_training=True)\n",
    "        print(f\"‚úì Loaded epoch: {trainer.start_epoch}\")\n",
    "        print(f\"‚úì Restored optimizer state\")\n",
    "        print(f\"‚úì Restored training history\")\n",
    "        \n",
    "        # Extend training duration if needed\n",
    "        if EXTEND_TRAINING_TO > trainer.num_epochs:\n",
    "            print(f\"\\nExtending training from {trainer.num_epochs} to {EXTEND_TRAINING_TO} epochs\")\n",
    "            trainer.num_epochs = EXTEND_TRAINING_TO\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to load checkpoint: {e}\")\n",
    "        print(\"Starting fresh training instead...\")\n",
    "        RESUME_TRAINING = False\n",
    "elif RESUME_TRAINING:\n",
    "    print(\"No checkpoint found - starting fresh training\")\n",
    "else:\n",
    "    print(\"Starting fresh training (resume=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e402d",
   "metadata": {},
   "source": [
    "## 8. Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "Training for 400 epochs (starting from epoch 341)...\n",
      "Training samples: 800, Validation samples: 200\n",
      "AP evaluation every 10 epochs\n",
      "\n",
      "Epoch 341/400 [Training...]\r"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Clear any remaining GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Execute training\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úó TRAINING FAILED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö† Training interrupted by user\")\n",
    "    print(\"Checkpoint may have been saved - you can resume from the latest one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eca397",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "metrics_file = checkpoints_dir / 'metrics.json'\n",
    "\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Plot losses\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(metrics['train_losses'], label='Train Loss', marker='o', markersize=3)\n",
    "    axes[0].plot(metrics['val_losses'], label='Val Loss', marker='s', markersize=3)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Progress')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AP metrics plot (if available)\n",
    "    if metrics.get('ap_history') and len(metrics['ap_history']) > 0:\n",
    "        ap_epochs = [x['epoch'] for x in metrics['ap_history']]\n",
    "        map_values = [x['mAP'] for x in metrics['ap_history']]\n",
    "        ap50_values = [x['AP50'] for x in metrics['ap_history']]\n",
    "        ap75_values = [x['AP75'] for x in metrics['ap_history']]\n",
    "        \n",
    "        axes[1].plot(ap_epochs, map_values, label='mAP', marker='o')\n",
    "        axes[1].plot(ap_epochs, ap50_values, label='AP50', marker='s')\n",
    "        axes[1].plot(ap_epochs, ap75_values, label='AP75', marker='^')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('AP Score')\n",
    "        axes[1].set_title('AP Metrics')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No AP metrics yet', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "        axes[1].set_title('AP Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoints_dir / 'training_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Final train loss: {metrics['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Final val loss: {metrics['val_losses'][-1]:.4f}\")\n",
    "    \n",
    "    if metrics.get('ap_history') and len(metrics['ap_history']) > 0:\n",
    "        latest_ap = metrics['ap_history'][-1]\n",
    "        print(f\"\\nLatest AP Metrics (Epoch {latest_ap['epoch']}):\")\n",
    "        print(f\"  mAP:  {latest_ap['mAP']:.3f}\")\n",
    "        print(f\"  AP50: {latest_ap['AP50']:.3f}\")\n",
    "        print(f\"  AP75: {latest_ap['AP75']:.3f}\")\n",
    "        print(f\"  mAR:  {latest_ap['mAR']:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"No metrics file found - training may not have started yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf2e7e",
   "metadata": {},
   "source": [
    "## üìã Usage Instructions\n",
    "\n",
    "### First Time Setup (Google Colab):\n",
    "1. **Run cells 1-3** (Environment, Mount Drive, Clone Repo, Install Dependencies)\n",
    "2. **‚ö†Ô∏è CRITICAL: Restart Runtime** (Runtime ‚Üí Restart session)\n",
    "3. **After restart, run cells 1-2 again** (to re-establish environment)\n",
    "4. **Skip cell 3** (dependencies already installed)\n",
    "5. **Continue from cell 4** (Package Verification) and run all remaining cells\n",
    "\n",
    "### To Resume Training:\n",
    "1. After setup, in cell 7 ensure:\n",
    "   - `RESUME_TRAINING = True`\n",
    "   - `RESUME_FROM_CHECKPOINT = None` (auto-detects latest) or specify a specific checkpoint\n",
    "   - `EXTEND_TRAINING_TO = 500` (or your desired total epochs)\n",
    "2. Run cells 8-10\n",
    "\n",
    "### To Start Fresh:\n",
    "1. In cell 7, set `RESUME_TRAINING = False`\n",
    "2. Run cells 8-10\n",
    "\n",
    "### Why Restart Runtime?\n",
    "The numpy/torchvision compatibility issue you encountered happens because torchvision's C extensions need to be loaded with the correct numpy version. Installing numpy isn't enough - you must restart to reload the Python interpreter.\n",
    "\n",
    "### Key Features:\n",
    "- **Auto-resume**: Automatically finds and loads the latest checkpoint\n",
    "- **Metrics preserved**: Training history is restored from metrics.json\n",
    "- **Flexible extension**: Easily extend training duration\n",
    "- **Memory optimized**: Uses expandable CUDA segments and automatic cleanup\n",
    "- **Dependency fixed**: Locks numpy==1.26.4 to prevent PyTorch conflicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
